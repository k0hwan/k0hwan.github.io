<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Kyunghwan Kim | ÍπÄÍ≤ΩÌôò</title> <meta name="author" content="Kyunghwan Kim"> <meta name="description" content="HCI Researcher. Ph.D. student at Human-Computer Interaction Lab (HCIL), School of Computing, KAIST. I connect Humans with Computers through novel Physical Interaction techniques!! "> <meta name="keywords" content="kyunghwan-kim, kaist, hci, hcil, homepage, chi, uist, masters-student, physical-interaction, interaction-techniques, extended-reality, xr"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://k0hwan.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Kyunghwan Kim | ÍπÄÍ≤ΩÌôò</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/patents/">Patents</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards</a> </li> <li class="nav-item "> <a class="nav-link" target="_blank" href="https://k0hwan.github.io/assets/pdf/CV_KyunghwanKim.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="bib-search-wrapper"> <div class="bib-search-container"> <svg class="search-icon" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg> <input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="bibsearch-form-input" placeholder="Search..."> </div> </div> <script src="/assets/js/bibsearch.js" type="module"></script> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#7030A0; margin-bottom: 2mm;">CHI LBW</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ball20-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ball20-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ball20-1400.webp"></source> <img src="/assets/img/publication_preview/ball20.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ball20.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3706599.3719967" class="col-sm-8"> <div> ¬† </div> <div class="title">Ball20: An In-Hand Near-Spherical 20-Sided Tangible Controller for Diverse Gesture Interaction in AR/VR</div> <div class="author"> Sunbum Kim,¬†<strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†Changsung Lim,¬†and¬†Geehyuk Lee</div> <div class="periodical"> <em>CHI ‚Äô25 LBW (Extended Abstracts)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1145/3706599.3719967" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/poster_ball20.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Spherical tangible devices have been explored to support effective object manipulation and enhance immersive experiences in augmented and virtual reality environments. However, because their spherical form makes it difficult to incorporate traditional input channels, their applicability and use as general-purpose input devices remain limited. In this paper, we present the Ball20, an in-hand near-spherical 20-sided tangible controller with independent force sensing on each face, designed to enable diverse gesture interactions. We developed the Ball20 hardware, designed a gesture set, and implemented a drawing application to demonstrate the Ball20 concept. In the first user study, we evaluated the feasibility of using the Ball20 for a drawing application and collected feedback. In the second user study, we further refined the Ball20 and conducted a quantitative usability evaluation.</p> </div> <div> ¬† </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#000000; margin-bottom: 2mm;">M.S. Thesis</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/msthesis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/msthesis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/msthesis-1400.webp"></source> <img src="/assets/img/publication_preview/msthesis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="msthesis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kyunghwan_ms" class="col-sm-8"> <div> ¬† </div> <div class="title">Virtual Rolling Temple (VRT): Expanding the Vertical Input Space of a Thin Smart Glasses Touchpad</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong> </div> <div class="periodical"> <em>School of Computing, KAIST - M.S. Thesis</em> </div> <div class="periodical"> </div> <div class="links"> <div> <b>Advisor</b>: Geehyuk Lee</div> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> </div> <div class="abstract hidden"> <p>We propose the Virtual Rolling Temple (VRT), a novel interaction method that enables precise pointing input on an expanded virtual plane by utilizing the entire finger instead of just the fingertip. The vertical input space is enlarged to the length of the finger, even on the limited physical space of a thin side touchpad of smart glasses. Study 1 evaluated the VRT interaction method compared to a wide touchpad of the same horizontal width. In the Target Selection, Straight Tunnel, and Half Circular Tunnel tasks, VRT achieved throughput rates of 87.7%, 75.2%, and 92.6% relative to the wide touchpad, demonstrating its potential. Subsequently, a displacement sensor-based hardware prototype was developed to substantiate the VRT concept within the form factor of smart glasses. Study 2 investigated how this prototype performs compared to the Optical Tracker-based GT condition with the same VRT input method. Across the same set of tasks as in Study 1, the prototype achieved throughput rates of 80.7%, 77.4%, and 77.6%, respectively. The throughput values of the VRT interaction method and prototype were comparable to or slightly lower than the results from touchpads on laptops and VR controllers in previous works. In conclusion, VRT is a promising, efficient, and practical on-device pointing input method, addressing the spatial limitations of thin smart glasses on-device touchpads.</p> </div> <div> ¬† </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#0E8288; margin-bottom: 2mm;">UIST</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/protact-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/protact-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/protact-1400.webp"></source> <img src="/assets/img/publication_preview/protact.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="protact.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3654777.3676324" class="col-sm-8"> <div> ¬† </div> <div class="title">Pro-Tact: Hierarchical Synthesis of Proprioception and Tactile Exploration for Eyes-Free Ray Pointing on Out-of-View VR Menus</div> <div class="author"> Yeonsu Kim,¬†Jisu Yim,¬†<strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†Yohan Yun,¬†and¬†Geehyuk Lee</div> <div class="periodical"> <em>UIST ‚Äô24</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1145/3654777.3676324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/fullHtml/10.1145/3654777.3676324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We introduce Pro-Tact, a novel eyes-free pointing technique for interacting with out-of-view (OoV) VR menus. This technique combines rapid rough pointing using proprioception with fine-grain adjustments through tactile exploration, enabling menu interaction without visual attention. Our user study demonstrated that Pro-Tact allows users to select menu items accurately (95% accuracy for 54 items) in an eyes-free manner, with reduced fatigue and sickness compared to eyes-engaged interaction. Additionally, we observed that participants voluntarily interacted with OoV menus eyes-free when Pro-Tact‚Äôs tactile feedback was provided in practical VR application usage contexts. This research contributes by introducing the novel interaction technique, Pro-Tact, and quantitatively evaluating its benefits in terms of performance, user experience, and user preference in OoV menu interactions.</p> </div> <div> ¬† </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#7030A0; margin-bottom: 2mm;">CHI Interactivity</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/stbutton-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/stbutton-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/stbutton-1400.webp"></source> <img src="/assets/img/publication_preview/stbutton.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="stbutton.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3613905.3648671" class="col-sm-8"> <div>üèÖ <b><span style="color: #FF6200;">Honorable Mention: Jury‚Äôs Best Demo Recognition</span></b> </div> <div class="title">STButton: Exploring Opportunities for Buttons with Spatio-Temporal Tactile Output</div> <div class="author"> Yeonsu Kim,¬†Jisu Yim,¬†Jaehyun Kim,¬†<strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†and¬†Geehyuk Lee</div> <div class="periodical"> <em>CHI ‚Äô24 Interactivity (Extended Abstracts)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1145/3613905.3648671" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/fullHtml/10.1145/3613905.3648671" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/poster_stbutton.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>We present STButton, a physical button with a high-resolution spatio-temporal tactile feedback surface. The 5 x 8 pin array tactile display size of 20mm x 28mm enables buttons to express various types of information, such as value with the number of raised pins, direction with the location of raised pins, and duration of time with blinking animation. With a highly expressive tactile surface, the button can seamlessly transfer assistive feedforward and feedback during spontaneous button interaction, such as touching to locate the button or applying gradual pressure to press the button. In the demonstration, attendees experience five scenarios of button interaction: the seat heater button on a car, the volume control button on a remote controller, the power button on a laptop, the menu button on a VR controller, and the play button on a game controller. In each scenario, the representative role of tactile feedback is configured differently, allowing attendees to experience the rich interaction space and potential benefits of STButton. Early accessed attendees appreciated the unique opportunity to transfer information with a highly expressive tactile surface and emphasized that STButton adds a tangible layer to the user experience, enhancing emotional and sensory engagement.</p> </div> <div> ¬† </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#1d3752; margin-bottom: 2mm;">Virtual Reality</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/rdw2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/rdw2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/rdw2-1400.webp"></source> <img src="/assets/img/publication_preview/rdw2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rdw2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1007/s10055-024-00997-y" class="col-sm-8"> <div> ¬† </div> <div class="title">Evaluation of visual, auditory, and olfactory stimulus-based attractors for intermittent reorientation in virtual reality locomotion</div> <div class="author"> Jieun Lee,¬†Seokhyun Hwang,¬†<strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†and¬†SeungJun Kim</div> <div class="periodical"> <em>Springer Virtual Reality ‚Äô24</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1007/s10055-024-00997-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In virtual reality, redirected walking (RDW) enables users to stay within the tracking area while feeling that they are traveling in a virtual space that is larger than the physical space. RDW uses a visual attractor to the user‚Äôs sight and scene manipulation for intermittent reorientation. However, repeated usage can hinder the virtual world immersion and weaken the reorientation performance. In this study, we propose using sounds and smells as alternative stimuli to draw the user‚Äôs attention implicitly and sustain the attractor‚Äôs performance for intermittent reorientation. To achieve this, we integrated visual, auditory, and olfactory attractors into an all-in-one stimulation system. Experiments revealed that the auditory attractor caused the fastest reorientation, the olfactory attractor induced the widest angular difference, and the attractor with the combined auditory and olfactory stimuli induced the largest angular speed, keeping users from noticing the manipulation. The findings demonstrate the potential of nonvisual attractors to reorient users in situations requiring intermittent reorientation.</p> </div> <div> ¬† </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#0E8288; margin-bottom: 2mm;">UIST Demo</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vrt-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vrt-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vrt-1400.webp"></source> <img src="/assets/img/publication_preview/vrt.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="vrt.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3586182.3615813" class="col-sm-8"> <div> ¬† </div> <div class="title">Virtual Rolling Temple: Expanding the Vertical Input Space of a Smart Glasses Touchpad</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†and¬†Geehyuk Lee</div> <div class="periodical"> <em>UIST ‚Äô23 Demo (Adjunct)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1145/3586182.3615813" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/fullHtml/10.1145/3586182.3615813" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/poster_vrt.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Smart glasses have not favored two-dimensional (2D) GUI. Such a trend may have originated from the limitations of smart glasses in display and input devices. While the display restriction is rapidly being resolved nowadays, 1D GUI is still the majority, indicating that the touch input device is the possible bottleneck. To tackle this issue by expanding the vertical input space of the temple touchpad, we propose the Virtual Rolling Temple (VRT). The concept is to perform 2D gestures by moving the hand in any direction while keep touching the prototype as if the temple rotates. The VRT touchpad is as thin as the spectacles‚Äô temples, but it provides the users with input space approximately equivalent to an 80√ó80 mm square touchpad. This is 8 and 13.9 times larger than Google Glass and VUZIX M400, respectively. To validate the concept of the VRT, we constructed three demo scenarios: 2D Pointing, 2D Menu, and 2D Gesture, to cover different types of general 2D input for smart glasses.</p> </div> <div> ¬† </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#000000; margin-bottom: 2mm;">B.S. Thesis</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bsthesis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bsthesis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bsthesis-1400.webp"></source> <img src="/assets/img/publication_preview/bsthesis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bsthesis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kyunghwan_bs" class="col-sm-8"> <div>üèÜ <b><span style="color: #FF6200;">Best B.S. Thesis Poster Presentation Award (1st place)</span></b> </div> <div class="title">Analysis of the Effect of Vection Generated by Directional Optical Flow in the VR Redirected Walking Scenario</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong> </div> <div class="periodical"> <em>School of Electrical Engineering and Computer Science (EECS), GIST College - B.S. Thesis</em> </div> <div class="periodical"> </div> <div class="links"> <div> <b>Advisor</b>: Jeany Son, <b>Co-Advisor</b>: SeungJun Kim</div> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="/assets/pdf/poster_bsthesis.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Human instinct is eager to explore the unexplored. The concept of Redirected Walking (RDW) was introduced to help with the aspiration of endeavoring to travel to a larger virtual environment by walking. However, compared to general walking scenarios in the real world, wearing Virtual Reality (VR) headsets negatively impacts gait stability. Adding RDW techniques to this would increase the mismatch between visual and vestibular information, resulting in even lower gait stability and possible motion sickness. This study aims to discover if vection generated by Optical Flow (OF) can help improve gait stability, enlarge Detection Threshold (DT) and relieve motion sickness. The results showed that adding vection to the RDW scenario hinders M/L gait stability while there was not much impact on A/P gait stability. Vection also slightly increased simulator sickness and decreased presence in a few combinations, and its relationship with DT was found to require a new measurement method.</p> </div> <div> ¬† </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2"> <abbr class="badge" style="background-color:#7030A0; margin-bottom: 2mm;">CHI LBW</abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/rdw-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/rdw-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/rdw-1400.webp"></source> <img src="/assets/img/publication_preview/rdw.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rdw.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3491101.3519719" class="col-sm-8"> <div> ¬† </div> <div class="title">Auditory and Olfactory Stimuli-Based Attractors to Induce Reorientation in Virtual Reality Forward Redirected Walking</div> <div class="author"> Jieun Lee,¬†Seokhyun Hwang,¬†<strong><span style="color:var(--global-theme-color)">Kyunghwan Kim</span></strong>,¬†and¬†SeungJun Kim</div> <div class="periodical"> <em>CHI ‚Äô22 LBW (Extended Abstracts)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://doi.org/10.1145/3491101.3519719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/fullHtml/10.1145/3491101.3519719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Redirected walking (RDW) visually manipulates the virtual environment to imperceptibly redirect the walkers to keep them in the tracking area, and offers a larger space than physical space. An attractor is a redirected walking technique that captures the walker‚Äôs attention and manipulates the walker‚Äôs trajectory through rotational gain. However, the attractor visually manipulates the walker‚Äôs virtual environment using a predefined rotational gain, and having to constantly gaze at the attractor or the attractor frequently appearing until the walker‚Äôs direction matches the desired direction, are problems limiting the application of visual attractors. Moreover, when the walker is unable to recognize or ignores the attractor, reorientation fails. In this study, we designed a human-sense-stimulating attractor that utilizes the auditory and olfactory senses to improve the rotational gain, naturalness, and immersion and decrease the chance of reorientation failure. Although sound and scents are invisible, they can be detected through direction; however, humans cannot recognize the accurate direction of a sound or scent. Based on these characteristics, auditory and olfactory attractors are proposed. We measured the amount of reorientation induced by the auditory and olfactory attractors and calculated the reorientation success rate. Additionally, the naturalness and immersion of the attractor were evaluated. The auditory attractor has a high reorientation success rate, naturalness, and immersion. The olfactory attractor induces more turn changes in the walker than other attractors, and a high number of turn changes leads to a larger rotational gain. Auditory and olfactory-based attractors have the potential to overcome the shortcomings of visual attractors such as the frequent interventions.</p> </div> <div> ¬† </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2026 Kyunghwan Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Last updated: February 10, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>